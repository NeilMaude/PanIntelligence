{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground\n",
    "This is a copy of the repeat call prediction coding.\n",
    "\n",
    "It is used to save time in development by retaining the model in memory\n",
    "\n",
    "For production, code will be transferred to a .py script to run as a scheduled task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Environment\n",
    "\n",
    "import sys              # General system functions\n",
    "import pyodbc           # Python ODBC library\n",
    "import pandas as pd     # Pandas data frame library\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler                    # for scaling features to zero-mean/unit-variance\n",
    "from sklearn.model_selection import train_test_split                # splitting of data into training/validation\n",
    "from keras.utils.np_utils import to_categorical                     # one-hot encoding of outputs\n",
    "from keras.models import Sequential                                 # Standard sequential model\n",
    "from keras.layers.core import Dense, Activation, Dropout            # Usual layers required\n",
    "from keras.optimizers import Adam                                   # Import the Adam optimiser\n",
    "from keras.callbacks import EarlyStopping                           # Early stopping callback to avoid over-fit\n",
    "# Constants\n",
    "MODULE_NAME = 'call_analysis.py'\n",
    "SERVER = 'wf-sql-01'\n",
    "DATABASE = 'smart'\n",
    "CONNECTION = 'DRIVER={SQL Server Native Client 11.0};'\n",
    "\n",
    "REPEAT_DAYS = 14        # Number of calendar days to allow for a repeat call\n",
    "TRAIN_SIZE = 0.8        # Percentage of training data to use for training, vs validation\n",
    "\n",
    "# Field handling\n",
    "# Categoricals are fields to 1-hot encode for deep learning\n",
    "categoricals = ['BusinessType','Manufacturer', 'ProductId', 'DeviceType','LastOtherCallType','CreatedBy','CreatedDay',\n",
    "                'AttendDay', 'PostCodeArea','FirstEngineer','SymptomCodeId']\n",
    "# Drop fields are fields to dispose of from the dataframe, as not useful for prediction\n",
    "drop_fields = ['ID', 'Incident', 'IncidentType', 'CustomerId', 'LedgerCode', 'SiteId',\n",
    "               'SerialNo', 'SymptomDescription']\n",
    "# Time stamp fields - these will also need to be dropped from the dataframe, they're already encoded\n",
    "time_stamp_fields = ['FirstUsed','Installed','LastBreakCall','LastOtherCall','InitialMeterReadingDate',\n",
    "                     'LastMeterReadingDate','CreatedDateTime','CreatedTime','AttendDateTime']\n",
    "\n",
    "# Global variables\n",
    "dbconn = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = 'ReportsReader'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the password for ReportsReader\n",
      "········\n"
     ]
    }
   ],
   "source": [
    "import getpass    # portable password input\n",
    "password = getpass.getpass('Enter the password for ' + user +'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function - just stick quotes around the string, for SQL building\n",
    "def quote_string(sString):\n",
    "    if sString.find(\"'\") < 0:\n",
    "        return \"'%s'\" % sString\n",
    "    else:\n",
    "        return \"'%s'\" % sString.replace(\"'\", \"#\")\n",
    "\n",
    "def writelog(message):\n",
    "\n",
    "    global dbconn\n",
    "\n",
    "    # write to the message log\n",
    "    sql = 'INSERT INTO zCALL_ANALYSIS_LOG (Description) VALUES (' + quote_string(message) + ')'\n",
    "    #print(sql)\n",
    "    cursor = dbconn.cursor()\n",
    "    cursor.execute(sql)\n",
    "    cursor.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting analysis process\n",
      "Read 30403 rows, with 57 columns\n",
      "Now have 30403 rows, with 1361 columns\n",
      "Minimum incidentId for prediction: 255705\n",
      "Found minimum incident at row 28333, incidentId 255705\n",
      "Retained incident list with 30403 rows\n",
      "After dropping fields, now have 30403 rows, with 1333 columns\n",
      "Created training set of size 22666, validation set of size 5667\n",
      "Will predict on 2070 calls since cut-off date\n",
      "Total records: 30403\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 1333)              1778222   \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 1333)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1333)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1000)              1334000   \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 200)               200200    \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 2)                 22        \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 3,338,104\n",
      "Trainable params: 3,338,104\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:107: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22666 samples, validate on 5667 samples\n",
      "Epoch 1/50\n",
      "22666/22666 [==============================] - 15s 651us/step - loss: 0.3501 - acc: 0.8613 - val_loss: 0.3158 - val_acc: 0.8692\n",
      "Epoch 2/50\n",
      "22666/22666 [==============================] - 11s 507us/step - loss: 0.2789 - acc: 0.8746 - val_loss: 0.2997 - val_acc: 0.8731\n",
      "Epoch 3/50\n",
      "22666/22666 [==============================] - 12s 510us/step - loss: 0.2496 - acc: 0.8833 - val_loss: 0.2934 - val_acc: 0.8767\n",
      "Epoch 4/50\n",
      "22666/22666 [==============================] - 12s 507us/step - loss: 0.2263 - acc: 0.8926 - val_loss: 0.3155 - val_acc: 0.8692\n",
      "Epoch 5/50\n",
      "22666/22666 [==============================] - 12s 508us/step - loss: 0.2079 - acc: 0.8999 - val_loss: 0.3360 - val_acc: 0.8742\n",
      "Epoch 6/50\n",
      "22666/22666 [==============================] - 12s 508us/step - loss: 0.1943 - acc: 0.9064 - val_loss: 0.3024 - val_acc: 0.8724\n",
      "Epoch 7/50\n",
      "22666/22666 [==============================] - 12s 508us/step - loss: 0.1768 - acc: 0.9148 - val_loss: 0.3064 - val_acc: 0.8761\n",
      "Epoch 8/50\n",
      "22666/22666 [==============================] - 11s 507us/step - loss: 0.1624 - acc: 0.9239 - val_loss: 0.4225 - val_acc: 0.8714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d660257da0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Starting analysis process')\n",
    "global dbconn\n",
    "\n",
    "# Connect to the database\n",
    "s_conn = CONNECTION + 'SERVER='+SERVER+';DATABASE='+DATABASE+';'\n",
    "s_conn += 'UID='+username+';PWD='+password\n",
    "dbconn = pyodbc.connect(s_conn)\n",
    "\n",
    "writelog('Python analysis started and connected to database OK')\n",
    "\n",
    "# Read in the data\n",
    "sql = 'select * from zCALL_ANALYSIS Order By [Incident] ASC'\n",
    "df_all = pd.read_sql_query(sql, dbconn)                         # can read SQL direct to a data frame\n",
    "print('Read %s rows, with %s columns' % (df_all.shape[0],df_all.shape[1]))\n",
    "\n",
    "# Encode the data\n",
    "# Need to set up unique names for values\n",
    "for c in categoricals:\n",
    "    df_all[c] = df_all[c].map(lambda x: ((str(c)) + '-' + str(x)))\n",
    "# Now go over all of the columns and create one-hot encoding\n",
    "for c in categoricals:\n",
    "    one_hot = pd.get_dummies(df_all[c])\n",
    "    df_all = df_all.join(one_hot)\n",
    "print('Now have %s rows, with %s columns' % (df_all.shape[0], df_all.shape[1]))\n",
    "# Extract the ground-truth values\n",
    "y_repeat = df_all['IncidentType'].map(lambda x: x == 'REPEAT')\n",
    "\n",
    "# Find the point at which predictions required (i.e recent incidents)\n",
    "sql = 'select MIN(Incident) [MinIncident] from zCALL_ANALYSIS '\n",
    "sql += 'where AttendDateTime >= DATEADD(d, -' + str(REPEAT_DAYS) + ', getdate())'\n",
    "cursor = dbconn.cursor()\n",
    "cursor.execute(sql)\n",
    "rs = cursor.fetchall()\n",
    "min_prediction_incident = rs[0].MinIncident\n",
    "\n",
    "print('Minimum incidentId for prediction: %s' % min_prediction_incident)\n",
    "\n",
    "min_prediction_index = df_all.shape[0]-1\n",
    "while (df_all['Incident'][min_prediction_index] > min_prediction_incident):\n",
    "    min_prediction_index -= 1\n",
    "print('Found minimum incident at row %s, incidentId %s' % (min_prediction_index, df_all['Incident'][min_prediction_index]))\n",
    "\n",
    "# Retain the list of incident IDs, for reporting later\n",
    "df_incident = df_all['Incident']\n",
    "print('Retained incident list with %s rows' % (df_incident.shape[0]))\n",
    "\n",
    "# Drop out fields which are not required\n",
    "for c in categoricals + drop_fields + time_stamp_fields:\n",
    "    del df_all[c]\n",
    "print('After dropping fields, now have %s rows, with %s columns' % (df_all.shape[0], df_all.shape[1]))\n",
    "\n",
    "# Get rid of any NULL values in numerical fields, replace any remaining NaN values with -1\n",
    "# This is to avoid breaking the algorithms later\n",
    "df_all = df_all.fillna(-1)\n",
    "\n",
    "# Scale the fields, zero-mean/unit-variance\n",
    "X_scaler = StandardScaler().fit(df_all)\n",
    "X_scaled = X_scaler.transform(df_all)\n",
    "\n",
    "# Encode the target outputs\n",
    "y_binary = to_categorical(y_repeat)\n",
    "\n",
    "# Split out the training data / test data / prediction data\n",
    "X_predict = X_scaled[min_prediction_index:]\n",
    "y_predict = y_binary[min_prediction_index:]                  # may use this for known REPEATS in window\n",
    "X_model   = X_scaled[0:min_prediction_index]\n",
    "y_model   = y_binary[0:min_prediction_index]\n",
    "\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X_model, y_model,\n",
    "                                                            train_size=TRAIN_SIZE, test_size=1-TRAIN_SIZE)\n",
    "\n",
    "print('Created training set of size %s, validation set of size %s' % (len(X_train), len(X_validate)))\n",
    "print('Will predict on %s calls since cut-off date' % len(X_predict))\n",
    "print('Total records: %s' % (len(X_train)+len(X_validate)+len(X_predict)))\n",
    "\n",
    "# Create the network structure\n",
    "nn_model = Sequential()\n",
    "nn_model.add(Dense(df_all.shape[1], input_shape=(df_all.shape[1] * 1,)))\n",
    "nn_model.add(Activation('relu'))\n",
    "nn_model.add(Dropout(0.5))\n",
    "nn_model.add(Dense(1000))\n",
    "nn_model.add(Activation('relu'))\n",
    "nn_model.add(Dense(200))\n",
    "nn_model.add(Activation('relu'))\n",
    "nn_model.add(Dense(100))\n",
    "nn_model.add(Activation('relu'))\n",
    "nn_model.add(Dense(50))\n",
    "nn_model.add(Activation('relu'))\n",
    "nn_model.add(Dense(10))\n",
    "nn_model.add(Activation('relu'))\n",
    "nn_model.add(Dense(2))\n",
    "nn_model.add(Activation('softmax'))\n",
    "nn_model.compile(loss='categorical_crossentropy',\n",
    "                 optimizer=Adam(),\n",
    "                 metrics=['accuracy'])\n",
    "nn_model.summary()\n",
    "\n",
    "# Train a model, with early stopping\n",
    "nBatchSize = 32\n",
    "nEpoch = 50\n",
    "early_stop = EarlyStopping(monitor='val_loss',\n",
    "                           min_delta=0,\n",
    "                           patience=5,\n",
    "                           verbose=0, mode='min')\n",
    "nn_model.fit(X_train, y_train,\n",
    "             batch_size=nBatchSize, epochs=nEpoch,\n",
    "             verbose=1, validation_data=(X_validate, y_validate), callbacks=[early_stop])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What does y_predict look like?\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item 12 is already a repeat\n",
      "Model will predict this with confidence:  0.9319371\n",
      "Item 15 is already a repeat\n",
      "Model will predict this with confidence:  0.0036825442\n",
      "Item 21 is already a repeat\n",
      "Model will predict this with confidence:  0.97741824\n",
      "Item 28 is already a repeat\n",
      "Model will predict this with confidence:  0.55748457\n",
      "Item 35 is already a repeat\n",
      "Model will predict this with confidence:  0.0372141\n",
      "Item 37 is already a repeat\n",
      "Model will predict this with confidence:  0.7121492\n",
      "Item 42 is already a repeat\n",
      "Model will predict this with confidence:  0.012182841\n",
      "Item 69 is already a repeat\n",
      "Model will predict this with confidence:  0.81635183\n",
      "Item 71 is already a repeat\n",
      "Model will predict this with confidence:  0.63231546\n",
      "Item 124 is already a repeat\n",
      "Model will predict this with confidence:  0.046593376\n"
     ]
    }
   ],
   "source": [
    "# Make predictions (note that some calls may already be repeats by this time)\n",
    "count = 0\n",
    "for r in range(len(X_predict)):\n",
    "    if y_predict[r][1] > 0.1:\n",
    "        print('Item ' + str(r) + ' is already a repeat')\n",
    "        count += 1\n",
    "        print('Model will predict this with confidence: ', nn_model.predict(X_predict[r:r+1])[0][1])\n",
    "    if count == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Done\n",
    "writelog('Python analysis process completed')\n",
    "print('\\nAnalysis process completed')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
