{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground\n",
    "This is a copy of the repeat call prediction coding.\n",
    "\n",
    "It is used to save time in development by retaining the model in memory\n",
    "\n",
    "For production, code will be transferred to a .py script to run as a scheduled task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Environment\n",
    "\n",
    "import sys              # General system functions\n",
    "import pyodbc           # Python ODBC library\n",
    "import pandas as pd     # Pandas data frame library\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler                    # for scaling features to zero-mean/unit-variance\n",
    "from sklearn.model_selection import train_test_split                # splitting of data into training/validation\n",
    "from keras.utils.np_utils import to_categorical                     # one-hot encoding of outputs\n",
    "from keras.models import Sequential                                 # Standard sequential model\n",
    "from keras.layers.core import Dense, Activation, Dropout            # Usual layers required\n",
    "from keras.optimizers import Adam                                   # Import the Adam optimiser\n",
    "from keras.callbacks import EarlyStopping                           # Early stopping callback to avoid over-fit\n",
    "# Constants\n",
    "MODULE_NAME = 'call_analysis.py'\n",
    "SERVER = 'wf-sql-01'\n",
    "DATABASE = 'smart'\n",
    "CONNECTION = 'DRIVER={SQL Server Native Client 11.0};'\n",
    "\n",
    "REPEAT_DAYS = 14        # Number of calendar days to allow for a repeat call\n",
    "TRAIN_SIZE = 0.8        # Percentage of training data to use for training, vs validation\n",
    "\n",
    "# Field handling\n",
    "# Categoricals are fields to 1-hot encode for deep learning\n",
    "categoricals = ['BusinessType','Manufacturer', 'ProductId', 'DeviceType','LastOtherCallType','CreatedBy','CreatedDay',\n",
    "                'AttendDay', 'PostCodeArea','FirstEngineer','SymptomCodeId']\n",
    "# Drop fields are fields to dispose of from the dataframe, as not useful for prediction\n",
    "drop_fields = ['Repeated','ID', 'Incident', 'IncidentType', 'CustomerId', 'LedgerCode', 'SiteId',\n",
    "               'SerialNo', 'SymptomDescription']\n",
    "# Time stamp fields - these will also need to be dropped from the dataframe, they're already encoded\n",
    "time_stamp_fields = ['FirstUsed','Installed','LastBreakCall','LastOtherCall','InitialMeterReadingDate',\n",
    "                     'LastMeterReadingDate','CreatedDateTime','CreatedTime','AttendDateTime']\n",
    "\n",
    "# Global variables\n",
    "dbconn = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = 'ReportsReader'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the password for ReportsReader\n",
      "········\n"
     ]
    }
   ],
   "source": [
    "import getpass    # portable password input\n",
    "password = getpass.getpass('Enter the password for ' + username +'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function - just stick quotes around the string, for SQL building\n",
    "def quote_string(sString):\n",
    "    if sString.find(\"'\") < 0:\n",
    "        return \"'%s'\" % sString\n",
    "    else:\n",
    "        return \"'%s'\" % sString.replace(\"'\", \"#\")\n",
    "\n",
    "def writelog(message):\n",
    "\n",
    "    global dbconn\n",
    "\n",
    "    # write to the message log\n",
    "    sql = 'INSERT INTO zCALL_ANALYSIS_LOG (Description) VALUES (' + quote_string(message) + ')'\n",
    "    #print(sql)\n",
    "    cursor = dbconn.cursor()\n",
    "    cursor.execute(sql)\n",
    "    cursor.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting analysis process\n",
      "Read 30755 rows, with 58 columns\n",
      "Now have 30755 rows, with 1365 columns\n",
      "Minimum incidentId for prediction: 258214\n",
      "Found minimum incident at row 29064, incidentId 258214\n",
      "Retained incident list with 30755 rows\n",
      "After dropping fields, now have 30755 rows, with 1336 columns\n",
      "Created training set of size 22451, validation set of size 5613\n",
      "Will predict on 1691 calls since cut-off date\n",
      "Total records: 29755\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1336)              1786232   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1336)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1336)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1000)              1337000   \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 200)               200200    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 22        \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 3,349,114\n",
      "Trainable params: 3,349,114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 22451 samples, validate on 5613 samples\n",
      "Epoch 1/50\n",
      "22451/22451 [==============================] - 15s 687us/step - loss: 0.5700 - acc: 0.7331 - val_loss: 0.5587 - val_acc: 0.7315\n",
      "Epoch 2/50\n",
      "22451/22451 [==============================] - 11s 495us/step - loss: 0.5389 - acc: 0.7409 - val_loss: 0.5595 - val_acc: 0.7315\n",
      "Epoch 3/50\n",
      "22451/22451 [==============================] - 11s 500us/step - loss: 0.5253 - acc: 0.7454 - val_loss: 0.5644 - val_acc: 0.7312\n",
      "Epoch 4/50\n",
      "22451/22451 [==============================] - 11s 490us/step - loss: 0.5154 - acc: 0.7470 - val_loss: 0.5681 - val_acc: 0.7269\n",
      "Epoch 5/50\n",
      "22451/22451 [==============================] - 11s 499us/step - loss: 0.5052 - acc: 0.7506 - val_loss: 0.5884 - val_acc: 0.7301\n",
      "Epoch 6/50\n",
      "22451/22451 [==============================] - 12s 517us/step - loss: 0.4898 - acc: 0.7580 - val_loss: 0.5896 - val_acc: 0.7283\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23f72fa7940>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Starting analysis process')\n",
    "global dbconn\n",
    "\n",
    "# Connect to the database\n",
    "s_conn = CONNECTION + 'SERVER='+SERVER+';DATABASE='+DATABASE+';'\n",
    "s_conn += 'UID='+username+';PWD='+password\n",
    "dbconn = pyodbc.connect(s_conn)\n",
    "\n",
    "writelog('Python analysis started and connected to database OK')\n",
    "\n",
    "# Read in the data\n",
    "sql = 'select * from zCALL_ANALYSIS Order By [Incident] ASC'\n",
    "df_all = pd.read_sql_query(sql, dbconn)                         # can read SQL direct to a data frame\n",
    "print('Read %s rows, with %s columns' % (df_all.shape[0],df_all.shape[1]))\n",
    "\n",
    "# Encode the data\n",
    "# Need to set up unique names for values\n",
    "for c in categoricals:\n",
    "    df_all[c] = df_all[c].map(lambda x: ((str(c)) + '-' + str(x)))\n",
    "# Now go over all of the columns and create one-hot encoding\n",
    "for c in categoricals:\n",
    "    one_hot = pd.get_dummies(df_all[c])\n",
    "    df_all = df_all.join(one_hot)\n",
    "print('Now have %s rows, with %s columns' % (df_all.shape[0], df_all.shape[1]))\n",
    "# Extract the ground-truth values\n",
    "#y_repeat = df_all['IncidentType'].map(lambda x: x == 'REPEAT')\n",
    "y_repeat = df_all['Repeated'].map(lambda x: x == 'YES')\n",
    "\n",
    "# Find the point at which predictions required (i.e recent incidents)\n",
    "sql = 'select MIN(Incident) [MinIncident] from zCALL_ANALYSIS '\n",
    "sql += 'where AttendDateTime >= DATEADD(d, -' + str(REPEAT_DAYS) + ', getdate())'\n",
    "cursor = dbconn.cursor()\n",
    "cursor.execute(sql)\n",
    "rs = cursor.fetchall()\n",
    "min_prediction_incident = rs[0].MinIncident\n",
    "\n",
    "print('Minimum incidentId for prediction: %s' % min_prediction_incident)\n",
    "\n",
    "min_prediction_index = df_all.shape[0]-1\n",
    "while (df_all['Incident'][min_prediction_index] > min_prediction_incident):\n",
    "    min_prediction_index -= 1\n",
    "print('Found minimum incident at row %s, incidentId %s' % (min_prediction_index, df_all['Incident'][min_prediction_index]))\n",
    "\n",
    "# Retain the list of incident IDs, for reporting later\n",
    "df_incident = df_all['Incident']\n",
    "print('Retained incident list with %s rows' % (df_incident.shape[0]))\n",
    "\n",
    "# Drop out fields which are not required\n",
    "for c in categoricals + drop_fields + time_stamp_fields:\n",
    "    del df_all[c]\n",
    "print('After dropping fields, now have %s rows, with %s columns' % (df_all.shape[0], df_all.shape[1]))\n",
    "\n",
    "# Get rid of any NULL values in numerical fields, replace any remaining NaN values with -1\n",
    "# This is to avoid breaking the algorithms later\n",
    "df_all = df_all.fillna(-1)\n",
    "\n",
    "# Scale the fields, zero-mean/unit-variance\n",
    "X_scaler = StandardScaler().fit(df_all)\n",
    "X_scaled = X_scaler.transform(df_all)\n",
    "\n",
    "# Encode the target outputs\n",
    "y_binary = to_categorical(y_repeat)\n",
    "\n",
    "# Split out the training data / test data / prediction data\n",
    "X_predict = X_scaled[min_prediction_index:]\n",
    "y_predict = y_binary[min_prediction_index:]                  # may use this for known REPEATS in window\n",
    "X_model   = X_scaled[0:min_prediction_index]\n",
    "y_model   = y_binary[0:min_prediction_index]\n",
    "\n",
    "# For playground, create an X_test set of 1000 items\n",
    "X_test = X_model[len(X_model)-1000:]\n",
    "y_test = y_model[len(y_model)-1000:]\n",
    "X_model = X_model[0:len(X_model)-1000]\n",
    "y_model = y_model[0:len(y_model)-1000]\n",
    "\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X_model, y_model,\n",
    "                                                            train_size=TRAIN_SIZE, test_size=1-TRAIN_SIZE)\n",
    "\n",
    "print('Created training set of size %s, validation set of size %s' % (len(X_train), len(X_validate)))\n",
    "print('Will predict on %s calls since cut-off date' % len(X_predict))\n",
    "print('Total records: %s' % (len(X_train)+len(X_validate)+len(X_predict)))\n",
    "\n",
    "# Create the network structure\n",
    "nn_model = Sequential()\n",
    "nn_model.add(Dense(df_all.shape[1], input_shape=(df_all.shape[1] * 1,)))\n",
    "nn_model.add(Activation('relu'))\n",
    "nn_model.add(Dropout(0.5))\n",
    "nn_model.add(Dense(1000))\n",
    "nn_model.add(Activation('relu'))\n",
    "nn_model.add(Dense(200))\n",
    "nn_model.add(Activation('relu'))\n",
    "nn_model.add(Dense(100))\n",
    "nn_model.add(Activation('relu'))\n",
    "nn_model.add(Dense(50))\n",
    "nn_model.add(Activation('relu'))\n",
    "nn_model.add(Dense(10))\n",
    "nn_model.add(Activation('relu'))\n",
    "nn_model.add(Dense(2))\n",
    "nn_model.add(Activation('softmax'))\n",
    "nn_model.compile(loss='categorical_crossentropy',\n",
    "                 optimizer=Adam(),\n",
    "                 metrics=['accuracy'])\n",
    "nn_model.summary()\n",
    "\n",
    "# Train a model, with early stopping\n",
    "nBatchSize = 32\n",
    "nEpoch = 50\n",
    "early_stop = EarlyStopping(monitor='val_loss',\n",
    "                           min_delta=0,\n",
    "                           patience=5,\n",
    "                           verbose=0, mode='min')\n",
    "nn_model.fit(X_train, y_train,\n",
    "             batch_size=nBatchSize, epochs=nEpoch,\n",
    "             verbose=1, validation_data=(X_validate, y_validate), callbacks=[early_stop])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What does y_predict look like?\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.62812114 0.37187883]\n",
      " [0.7939698  0.20603019]\n",
      " [0.52275115 0.47724888]\n",
      " [0.69188607 0.30811387]\n",
      " [0.70654655 0.29345345]\n",
      " [0.9829374  0.0170626 ]\n",
      " [0.6291801  0.37081996]\n",
      " [0.73517096 0.26482907]\n",
      " [0.7662988  0.23370121]\n",
      " [0.61422396 0.385776  ]]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Test accuracy\n",
    "# Use X_test / y_test data - will be known repeats\n",
    "import numpy as np\n",
    "predictions = nn_model.predict(X_test)\n",
    "predicted_repeats = np.argmax(predictions,axis=1)\n",
    "print(predictions[0:10])\n",
    "print(predicted_repeats[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results are:\n",
      " True positives  - correct predictions:   62\n",
      " True negatives  - correct predictions:   623\n",
      " False positives - incorrect predictions: 38\n",
      " False negatives - incorrect predictions: 277\n"
     ]
    }
   ],
   "source": [
    "# performance on plain argmax of confidence levels\n",
    "true_positive = 0\n",
    "false_positive = 0\n",
    "true_negative = 0\n",
    "false_negative = 0\n",
    "for i in range(len(predictions)):\n",
    "    if y_test[i][1] > 0.1:\n",
    "        # the ground truth is a repeat\n",
    "        if predicted_repeats[i] == 1:\n",
    "            true_positive += 1\n",
    "        else:\n",
    "            false_negative += 1\n",
    "    else:\n",
    "        # the ground truth is not a repeat\n",
    "        if predicted_repeats[i] == 1:\n",
    "            false_positive += 1\n",
    "        else:\n",
    "            true_negative += 1 \n",
    "print('Results are:')\n",
    "print(' True positives  - correct predictions:   %s' % true_positive)\n",
    "print(' True negatives  - correct predictions:   %s' % true_negative)\n",
    "print(' False positives - incorrect predictions: %s' % false_positive)\n",
    "print(' False negatives - incorrect predictions: %s' % false_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold setting = 0.05\n",
      " True positives  - correct predictions:   325\n",
      " True negatives  - correct predictions:   57\n",
      " False positives - incorrect predictions: 604\n",
      " False negatives - incorrect predictions: 14\n",
      "Threshold setting = 0.10\n",
      " True positives  - correct predictions:   311\n",
      " True negatives  - correct predictions:   115\n",
      " False positives - incorrect predictions: 546\n",
      " False negatives - incorrect predictions: 28\n",
      "Threshold setting = 0.15\n",
      " True positives  - correct predictions:   295\n",
      " True negatives  - correct predictions:   179\n",
      " False positives - incorrect predictions: 482\n",
      " False negatives - incorrect predictions: 44\n",
      "Threshold setting = 0.20\n",
      " True positives  - correct predictions:   279\n",
      " True negatives  - correct predictions:   237\n",
      " False positives - incorrect predictions: 424\n",
      " False negatives - incorrect predictions: 60\n",
      "Threshold setting = 0.25\n",
      " True positives  - correct predictions:   260\n",
      " True negatives  - correct predictions:   298\n",
      " False positives - incorrect predictions: 363\n",
      " False negatives - incorrect predictions: 79\n",
      "Threshold setting = 0.30\n",
      " True positives  - correct predictions:   230\n",
      " True negatives  - correct predictions:   361\n",
      " False positives - incorrect predictions: 300\n",
      " False negatives - incorrect predictions: 109\n",
      "Threshold setting = 0.35\n",
      " True positives  - correct predictions:   176\n",
      " True negatives  - correct predictions:   461\n",
      " False positives - incorrect predictions: 200\n",
      " False negatives - incorrect predictions: 163\n",
      "Threshold setting = 0.40\n",
      " True positives  - correct predictions:   112\n",
      " True negatives  - correct predictions:   541\n",
      " False positives - incorrect predictions: 120\n",
      " False negatives - incorrect predictions: 227\n",
      "Threshold setting = 0.45\n",
      " True positives  - correct predictions:   80\n",
      " True negatives  - correct predictions:   597\n",
      " False positives - incorrect predictions: 64\n",
      " False negatives - incorrect predictions: 259\n",
      "Threshold setting = 0.50\n",
      " True positives  - correct predictions:   62\n",
      " True negatives  - correct predictions:   623\n",
      " False positives - incorrect predictions: 38\n",
      " False negatives - incorrect predictions: 277\n",
      "Threshold setting = 0.55\n",
      " True positives  - correct predictions:   49\n",
      " True negatives  - correct predictions:   639\n",
      " False positives - incorrect predictions: 22\n",
      " False negatives - incorrect predictions: 290\n",
      "Threshold setting = 0.60\n",
      " True positives  - correct predictions:   37\n",
      " True negatives  - correct predictions:   647\n",
      " False positives - incorrect predictions: 14\n",
      " False negatives - incorrect predictions: 302\n",
      "Threshold setting = 0.65\n",
      " True positives  - correct predictions:   34\n",
      " True negatives  - correct predictions:   653\n",
      " False positives - incorrect predictions: 8\n",
      " False negatives - incorrect predictions: 305\n",
      "Threshold setting = 0.70\n",
      " True positives  - correct predictions:   25\n",
      " True negatives  - correct predictions:   657\n",
      " False positives - incorrect predictions: 4\n",
      " False negatives - incorrect predictions: 314\n",
      "Threshold setting = 0.75\n",
      " True positives  - correct predictions:   20\n",
      " True negatives  - correct predictions:   659\n",
      " False positives - incorrect predictions: 2\n",
      " False negatives - incorrect predictions: 319\n",
      "Threshold setting = 0.80\n",
      " True positives  - correct predictions:   14\n",
      " True negatives  - correct predictions:   660\n",
      " False positives - incorrect predictions: 1\n",
      " False negatives - incorrect predictions: 325\n",
      "Threshold setting = 0.85\n",
      " True positives  - correct predictions:   13\n",
      " True negatives  - correct predictions:   661\n",
      " False positives - incorrect predictions: 0\n",
      " False negatives - incorrect predictions: 326\n",
      "Threshold setting = 0.90\n",
      " True positives  - correct predictions:   11\n",
      " True negatives  - correct predictions:   661\n",
      " False positives - incorrect predictions: 0\n",
      " False negatives - incorrect predictions: 328\n",
      "Threshold setting = 0.95\n",
      " True positives  - correct predictions:   11\n",
      " True negatives  - correct predictions:   661\n",
      " False positives - incorrect predictions: 0\n",
      " False negatives - incorrect predictions: 328\n"
     ]
    }
   ],
   "source": [
    "# test thresholds, with creation of an output file\n",
    "file_name = 'results.csv'\n",
    "with open(file_name,'w') as file:\n",
    "    line = 'TruePositive,TrueNegative,FalsePositive,FalseNegative,Threshold\\n'\n",
    "    file.write(line)\n",
    "    for t in range(1,20,1):\n",
    "        threshold = float(t) / 20\n",
    "        true_positive = 0\n",
    "        false_positive = 0\n",
    "        true_negative = 0\n",
    "        false_negative = 0\n",
    "        for i in range(len(predictions)):\n",
    "            if y_test[i][1] > 0.1:\n",
    "                # the ground truth is a repeat\n",
    "                if predictions[i][1] >= threshold:\n",
    "                    true_positive += 1\n",
    "                else:\n",
    "                    false_negative += 1\n",
    "            else:\n",
    "                # the ground truth is not a repeat\n",
    "                if predictions[i][1] >= threshold:\n",
    "                    false_positive += 1\n",
    "                else:\n",
    "                    true_negative += 1 \n",
    "        print('Threshold setting = %.2f' % threshold)\n",
    "        print(' True positives  - correct predictions:   %s' % true_positive)\n",
    "        print(' True negatives  - correct predictions:   %s' % true_negative)\n",
    "        print(' False positives - incorrect predictions: %s' % false_positive)\n",
    "        print(' False negatives - incorrect predictions: %s' % false_negative)\n",
    "        line = str(true_positive) + ',' + str(true_negative) + ',' + str(false_positive) + ',' + str(false_negative) \n",
    "        line += ',' + str(threshold) + '\\n'\n",
    "        file.write(line)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item 0 is already a repeat\n",
      "Model will predict this with confidence:  0.35811403\n",
      "Item 6 is already a repeat\n",
      "Model will predict this with confidence:  0.1866563\n",
      "Item 7 is already a repeat\n",
      "Model will predict this with confidence:  0.32053506\n",
      "Item 11 is already a repeat\n",
      "Model will predict this with confidence:  0.57996505\n",
      "Item 16 is already a repeat\n",
      "Model will predict this with confidence:  0.34738135\n",
      "Item 17 is already a repeat\n",
      "Model will predict this with confidence:  0.9962388\n",
      "Item 18 is already a repeat\n",
      "Model will predict this with confidence:  0.99627435\n",
      "Item 20 is already a repeat\n",
      "Model will predict this with confidence:  0.48146048\n",
      "Item 21 is already a repeat\n",
      "Model will predict this with confidence:  0.072711065\n",
      "Item 22 is already a repeat\n",
      "Model will predict this with confidence:  0.12590216\n"
     ]
    }
   ],
   "source": [
    "# Make predictions (note that some calls may already be repeats by this time)\n",
    "count = 0\n",
    "for r in range(len(X_predict)):\n",
    "    if y_predict[r][1] > 0.1:\n",
    "        print('Item ' + str(r) + ' is already a repeat')\n",
    "        count += 1\n",
    "        print('Model will predict this with confidence: ', nn_model.predict(X_predict[r:r+1])[0][1])\n",
    "    if count == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'08-October-2018'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "dt = datetime.now()\n",
    "'{:%d-%B-%Y}'.format(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-01 00:00:00\n",
      "2018-10-02\n",
      "2018-10-02 23:59:59\n",
      "08/10/2018\n"
     ]
    }
   ],
   "source": [
    "from dateutil import parser\n",
    "dt = parser.parse('1 Oct 2018')\n",
    "print(str(dt))\n",
    "dt2 = datetime.strptime('02/10/2018', '%d/%m/%Y')\n",
    "print(str(dt2)[0:10])\n",
    "from datetime import timedelta\n",
    "dt3 = dt2 + timedelta(days=1) - timedelta(seconds=1)\n",
    "print(str(dt3))\n",
    "predict_date = datetime.now()\n",
    "print('{:%d/%m/%Y}'.format(predict_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Done\n",
    "writelog('Python analysis process completed')\n",
    "print('\\nAnalysis process completed')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
