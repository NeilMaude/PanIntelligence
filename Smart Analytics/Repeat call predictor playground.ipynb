{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground\n",
    "This is a copy of the repeat call prediction coding.\n",
    "\n",
    "It is used to save time in development by retaining the model in memory\n",
    "\n",
    "For production, code will be transferred to a .py script to run as a scheduled task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Environment\n",
    "\n",
    "import sys              # General system functions\n",
    "import pyodbc           # Python ODBC library\n",
    "import pandas as pd     # Pandas data frame library\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler                    # for scaling features to zero-mean/unit-variance\n",
    "from sklearn.model_selection import train_test_split                # splitting of data into training/validation\n",
    "from keras.utils.np_utils import to_categorical                     # one-hot encoding of outputs\n",
    "from keras.models import Sequential                                 # Standard sequential model\n",
    "from keras.layers.core import Dense, Activation, Dropout            # Usual layers required\n",
    "from keras.optimizers import Adam                                   # Import the Adam optimiser\n",
    "from keras.callbacks import EarlyStopping                           # Early stopping callback to avoid over-fit\n",
    "# Constants\n",
    "MODULE_NAME = 'call_analysis.py'\n",
    "SERVER = 'wf-sql-01'\n",
    "DATABASE = 'smart'\n",
    "CONNECTION = 'DRIVER={SQL Server Native Client 11.0};'\n",
    "\n",
    "REPEAT_DAYS = 14        # Number of calendar days to allow for a repeat call\n",
    "TRAIN_SIZE = 0.8        # Percentage of training data to use for training, vs validation\n",
    "\n",
    "# Field handling\n",
    "# Categoricals are fields to 1-hot encode for deep learning\n",
    "categoricals = ['BusinessType','Manufacturer', 'ProductId', 'DeviceType','LastOtherCallType','CreatedBy','CreatedDay',\n",
    "                'AttendDay', 'PostCodeArea','FirstEngineer','SymptomCodeId']\n",
    "# Drop fields are fields to dispose of from the dataframe, as not useful for prediction\n",
    "drop_fields = ['Repeated','ID', 'Incident', 'IncidentType', 'CustomerId', 'LedgerCode', 'SiteId',\n",
    "               'SerialNo', 'SymptomDescription']\n",
    "# Time stamp fields - these will also need to be dropped from the dataframe, they're already encoded\n",
    "time_stamp_fields = ['FirstUsed','Installed','LastBreakCall','LastOtherCall','InitialMeterReadingDate',\n",
    "                     'LastMeterReadingDate','CreatedDateTime','CreatedTime','AttendDateTime']\n",
    "\n",
    "# Global variables\n",
    "dbconn = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = 'ReportsReader'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the password for ReportsReader\n",
      "········\n"
     ]
    }
   ],
   "source": [
    "import getpass    # portable password input\n",
    "password = getpass.getpass('Enter the password for ' + username +'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function - just stick quotes around the string, for SQL building\n",
    "def quote_string(sString):\n",
    "    if sString.find(\"'\") < 0:\n",
    "        return \"'%s'\" % sString\n",
    "    else:\n",
    "        return \"'%s'\" % sString.replace(\"'\", \"#\")\n",
    "\n",
    "def writelog(message):\n",
    "\n",
    "    global dbconn\n",
    "\n",
    "    # write to the message log\n",
    "    sql = 'INSERT INTO zCALL_ANALYSIS_LOG (Description) VALUES (' + quote_string(message) + ')'\n",
    "    #print(sql)\n",
    "    cursor = dbconn.cursor()\n",
    "    cursor.execute(sql)\n",
    "    cursor.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting analysis process\n",
      "Read 30466 rows, with 58 columns\n",
      "Now have 30466 rows, with 1364 columns\n",
      "Minimum incidentId for prediction: 255705\n",
      "Found minimum incident at row 28333, incidentId 255705\n",
      "Retained incident list with 30466 rows\n",
      "After dropping fields, now have 30466 rows, with 1335 columns\n",
      "Created training set of size 21866, validation set of size 5467\n",
      "Will predict on 2133 calls since cut-off date\n",
      "Total records: 29466\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 1335)              1783560   \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 1335)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1335)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1000)              1336000   \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 200)               200200    \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 2)                 22        \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 3,345,442\n",
      "Trainable params: 3,345,442\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 21866 samples, validate on 5467 samples\n",
      "Epoch 1/50\n",
      "21866/21866 [==============================] - 12s 557us/step - loss: 0.5713 - acc: 0.7321 - val_loss: 0.5552 - val_acc: 0.7355 0.5946 - - ETA: 8s - loss: 0.5861 - - ETA: 7s - loss: 0.5868 - acc -  - ETA: 4s - loss: 0.5788  - ETA: 3s - loss: 0.5782 - acc: 0. - ETA: 2s - loss: 0.5767 - acc:  - ETA: 2s - ETA: 0s - loss: 0.5724 - acc:\n",
      "Epoch 2/50\n",
      "21866/21866 [==============================] - 11s 519us/step - loss: 0.5423 - acc: 0.7396 - val_loss: 0.5597 - val_acc: 0.7355 - acc: 0.73 - ETA: 4s - loss: 0.5424 - ETA: 3s - loss: - ETA: 0s - loss: 0.5421 - acc: \n",
      "Epoch 3/50\n",
      "21866/21866 [==============================] - 12s 528us/step - loss: 0.5295 - acc: 0.7428 - val_loss: 0.5616 - val_acc: 0.7339 - loss: 0.5270 - ETA:\n",
      "Epoch 4/50\n",
      "21866/21866 [==============================] - 11s 521us/step - loss: 0.5187 - acc: 0.7440 - val_loss: 0.5710 - val_acc: 0.7227\n",
      "Epoch 5/50\n",
      "21866/21866 [==============================] - 12s 526us/step - loss: 0.5074 - acc: 0.7492 - val_loss: 0.5744 - val_acc: 0.73062s - loss:  - ETA: 1s - loss: 0.5064 - acc: 0.748 - ETA: 1s - loss: 0.5061 - acc: 0.7 - ETA: 1s - lo\n",
      "Epoch 6/50\n",
      "21866/21866 [==============================] - 12s 526us/step - loss: 0.4937 - acc: 0.7520 - val_loss: 0.5796 - val_acc: 0.7271 0.4927 - acc: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b545688668>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Starting analysis process')\n",
    "global dbconn\n",
    "\n",
    "# Connect to the database\n",
    "s_conn = CONNECTION + 'SERVER='+SERVER+';DATABASE='+DATABASE+';'\n",
    "s_conn += 'UID='+username+';PWD='+password\n",
    "dbconn = pyodbc.connect(s_conn)\n",
    "\n",
    "writelog('Python analysis started and connected to database OK')\n",
    "\n",
    "# Read in the data\n",
    "sql = 'select * from zCALL_ANALYSIS Order By [Incident] ASC'\n",
    "df_all = pd.read_sql_query(sql, dbconn)                         # can read SQL direct to a data frame\n",
    "print('Read %s rows, with %s columns' % (df_all.shape[0],df_all.shape[1]))\n",
    "\n",
    "# Encode the data\n",
    "# Need to set up unique names for values\n",
    "for c in categoricals:\n",
    "    df_all[c] = df_all[c].map(lambda x: ((str(c)) + '-' + str(x)))\n",
    "# Now go over all of the columns and create one-hot encoding\n",
    "for c in categoricals:\n",
    "    one_hot = pd.get_dummies(df_all[c])\n",
    "    df_all = df_all.join(one_hot)\n",
    "print('Now have %s rows, with %s columns' % (df_all.shape[0], df_all.shape[1]))\n",
    "# Extract the ground-truth values\n",
    "#y_repeat = df_all['IncidentType'].map(lambda x: x == 'REPEAT')\n",
    "y_repeat = df_all['Repeated'].map(lambda x: x == 'YES')\n",
    "\n",
    "# Find the point at which predictions required (i.e recent incidents)\n",
    "sql = 'select MIN(Incident) [MinIncident] from zCALL_ANALYSIS '\n",
    "sql += 'where AttendDateTime >= DATEADD(d, -' + str(REPEAT_DAYS) + ', getdate())'\n",
    "cursor = dbconn.cursor()\n",
    "cursor.execute(sql)\n",
    "rs = cursor.fetchall()\n",
    "min_prediction_incident = rs[0].MinIncident\n",
    "\n",
    "print('Minimum incidentId for prediction: %s' % min_prediction_incident)\n",
    "\n",
    "min_prediction_index = df_all.shape[0]-1\n",
    "while (df_all['Incident'][min_prediction_index] > min_prediction_incident):\n",
    "    min_prediction_index -= 1\n",
    "print('Found minimum incident at row %s, incidentId %s' % (min_prediction_index, df_all['Incident'][min_prediction_index]))\n",
    "\n",
    "# Retain the list of incident IDs, for reporting later\n",
    "df_incident = df_all['Incident']\n",
    "print('Retained incident list with %s rows' % (df_incident.shape[0]))\n",
    "\n",
    "# Drop out fields which are not required\n",
    "for c in categoricals + drop_fields + time_stamp_fields:\n",
    "    del df_all[c]\n",
    "print('After dropping fields, now have %s rows, with %s columns' % (df_all.shape[0], df_all.shape[1]))\n",
    "\n",
    "# Get rid of any NULL values in numerical fields, replace any remaining NaN values with -1\n",
    "# This is to avoid breaking the algorithms later\n",
    "df_all = df_all.fillna(-1)\n",
    "\n",
    "# Scale the fields, zero-mean/unit-variance\n",
    "X_scaler = StandardScaler().fit(df_all)\n",
    "X_scaled = X_scaler.transform(df_all)\n",
    "\n",
    "# Encode the target outputs\n",
    "y_binary = to_categorical(y_repeat)\n",
    "\n",
    "# Split out the training data / test data / prediction data\n",
    "X_predict = X_scaled[min_prediction_index:]\n",
    "y_predict = y_binary[min_prediction_index:]                  # may use this for known REPEATS in window\n",
    "X_model   = X_scaled[0:min_prediction_index]\n",
    "y_model   = y_binary[0:min_prediction_index]\n",
    "\n",
    "# For playground, create an X_test set of 1000 items\n",
    "X_test = X_model[len(X_model)-1000:]\n",
    "y_test = y_model[len(y_model)-1000:]\n",
    "X_model = X_model[0:len(X_model)-1000]\n",
    "y_model = y_model[0:len(y_model)-1000]\n",
    "\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X_model, y_model,\n",
    "                                                            train_size=TRAIN_SIZE, test_size=1-TRAIN_SIZE)\n",
    "\n",
    "print('Created training set of size %s, validation set of size %s' % (len(X_train), len(X_validate)))\n",
    "print('Will predict on %s calls since cut-off date' % len(X_predict))\n",
    "print('Total records: %s' % (len(X_train)+len(X_validate)+len(X_predict)))\n",
    "\n",
    "# Create the network structure\n",
    "nn_model = Sequential()\n",
    "nn_model.add(Dense(df_all.shape[1], input_shape=(df_all.shape[1] * 1,)))\n",
    "nn_model.add(Activation('relu'))\n",
    "nn_model.add(Dropout(0.5))\n",
    "nn_model.add(Dense(1000))\n",
    "nn_model.add(Activation('relu'))\n",
    "nn_model.add(Dense(200))\n",
    "nn_model.add(Activation('relu'))\n",
    "nn_model.add(Dense(100))\n",
    "nn_model.add(Activation('relu'))\n",
    "nn_model.add(Dense(50))\n",
    "nn_model.add(Activation('relu'))\n",
    "nn_model.add(Dense(10))\n",
    "nn_model.add(Activation('relu'))\n",
    "nn_model.add(Dense(2))\n",
    "nn_model.add(Activation('softmax'))\n",
    "nn_model.compile(loss='categorical_crossentropy',\n",
    "                 optimizer=Adam(),\n",
    "                 metrics=['accuracy'])\n",
    "nn_model.summary()\n",
    "\n",
    "# Train a model, with early stopping\n",
    "nBatchSize = 32\n",
    "nEpoch = 50\n",
    "early_stop = EarlyStopping(monitor='val_loss',\n",
    "                           min_delta=0,\n",
    "                           patience=5,\n",
    "                           verbose=0, mode='min')\n",
    "nn_model.fit(X_train, y_train,\n",
    "             batch_size=nBatchSize, epochs=nEpoch,\n",
    "             verbose=1, validation_data=(X_validate, y_validate), callbacks=[early_stop])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What does y_predict look like?\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.81726134 0.18273865]\n",
      " [0.72780913 0.27219087]\n",
      " [0.7037798  0.29622015]\n",
      " [0.4561587  0.54384136]\n",
      " [0.9917557  0.00824425]\n",
      " [0.7368154  0.2631846 ]\n",
      " [0.6708834  0.3291166 ]\n",
      " [0.48952138 0.5104786 ]\n",
      " [0.7305655  0.26943448]\n",
      " [0.71038246 0.2896175 ]]\n",
      "[0 0 0 1 0 0 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Test accuracy\n",
    "# Use X_test / y_test data - will be known repeats\n",
    "import numpy as np\n",
    "predictions = nn_model.predict(X_test)\n",
    "predicted_repeats = np.argmax(predictions,axis=1)\n",
    "print(predictions[0:10])\n",
    "print(predicted_repeats[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results are:\n",
      " True positives  - correct predictions:   62\n",
      " True negatives  - correct predictions:   681\n",
      " False positives - incorrect predictions: 32\n",
      " False negatives - correct predictions:   225\n"
     ]
    }
   ],
   "source": [
    "# performance on plain argmax of confidence levels\n",
    "true_positive = 0\n",
    "false_positive = 0\n",
    "true_negative = 0\n",
    "false_negative = 0\n",
    "for i in range(len(predictions)):\n",
    "    if y_test[i][1] > 0.1:\n",
    "        # the ground truth is a repeat\n",
    "        if predicted_repeats[i] == 1:\n",
    "            true_positive += 1\n",
    "        else:\n",
    "            false_negative += 1\n",
    "    else:\n",
    "        # the ground truth is not a repeat\n",
    "        if predicted_repeats[i] == 1:\n",
    "            false_positive += 1\n",
    "        else:\n",
    "            true_negative += 1 \n",
    "print('Results are:')\n",
    "print(' True positives  - correct predictions:   %s' % true_positive)\n",
    "print(' True negatives  - correct predictions:   %s' % true_negative)\n",
    "print(' False positives - incorrect predictions: %s' % false_positive)\n",
    "print(' False negatives - incorrect predictions: %s' % false_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold setting = 0.05\n",
      " True positives  - correct predictions:   267\n",
      " True negatives  - correct predictions:   104\n",
      " False positives - incorrect predictions: 609\n",
      " False negatives - incorrect predictions: 20\n",
      "Threshold setting = 0.10\n",
      " True positives  - correct predictions:   253\n",
      " True negatives  - correct predictions:   181\n",
      " False positives - incorrect predictions: 532\n",
      " False negatives - incorrect predictions: 34\n",
      "Threshold setting = 0.15\n",
      " True positives  - correct predictions:   239\n",
      " True negatives  - correct predictions:   254\n",
      " False positives - incorrect predictions: 459\n",
      " False negatives - incorrect predictions: 48\n",
      "Threshold setting = 0.20\n",
      " True positives  - correct predictions:   213\n",
      " True negatives  - correct predictions:   322\n",
      " False positives - incorrect predictions: 391\n",
      " False negatives - incorrect predictions: 74\n",
      "Threshold setting = 0.25\n",
      " True positives  - correct predictions:   194\n",
      " True negatives  - correct predictions:   407\n",
      " False positives - incorrect predictions: 306\n",
      " False negatives - incorrect predictions: 93\n",
      "Threshold setting = 0.30\n",
      " True positives  - correct predictions:   149\n",
      " True negatives  - correct predictions:   507\n",
      " False positives - incorrect predictions: 206\n",
      " False negatives - incorrect predictions: 138\n",
      "Threshold setting = 0.35\n",
      " True positives  - correct predictions:   101\n",
      " True negatives  - correct predictions:   611\n",
      " False positives - incorrect predictions: 102\n",
      " False negatives - incorrect predictions: 186\n",
      "Threshold setting = 0.40\n",
      " True positives  - correct predictions:   83\n",
      " True negatives  - correct predictions:   646\n",
      " False positives - incorrect predictions: 67\n",
      " False negatives - incorrect predictions: 204\n",
      "Threshold setting = 0.45\n",
      " True positives  - correct predictions:   78\n",
      " True negatives  - correct predictions:   656\n",
      " False positives - incorrect predictions: 57\n",
      " False negatives - incorrect predictions: 209\n",
      "Threshold setting = 0.50\n",
      " True positives  - correct predictions:   62\n",
      " True negatives  - correct predictions:   681\n",
      " False positives - incorrect predictions: 32\n",
      " False negatives - incorrect predictions: 225\n",
      "Threshold setting = 0.55\n",
      " True positives  - correct predictions:   55\n",
      " True negatives  - correct predictions:   695\n",
      " False positives - incorrect predictions: 18\n",
      " False negatives - incorrect predictions: 232\n",
      "Threshold setting = 0.60\n",
      " True positives  - correct predictions:   48\n",
      " True negatives  - correct predictions:   699\n",
      " False positives - incorrect predictions: 14\n",
      " False negatives - incorrect predictions: 239\n",
      "Threshold setting = 0.65\n",
      " True positives  - correct predictions:   41\n",
      " True negatives  - correct predictions:   707\n",
      " False positives - incorrect predictions: 6\n",
      " False negatives - incorrect predictions: 246\n",
      "Threshold setting = 0.70\n",
      " True positives  - correct predictions:   32\n",
      " True negatives  - correct predictions:   709\n",
      " False positives - incorrect predictions: 4\n",
      " False negatives - incorrect predictions: 255\n",
      "Threshold setting = 0.75\n",
      " True positives  - correct predictions:   30\n",
      " True negatives  - correct predictions:   709\n",
      " False positives - incorrect predictions: 4\n",
      " False negatives - incorrect predictions: 257\n",
      "Threshold setting = 0.80\n",
      " True positives  - correct predictions:   27\n",
      " True negatives  - correct predictions:   710\n",
      " False positives - incorrect predictions: 3\n",
      " False negatives - incorrect predictions: 260\n",
      "Threshold setting = 0.85\n",
      " True positives  - correct predictions:   27\n",
      " True negatives  - correct predictions:   710\n",
      " False positives - incorrect predictions: 3\n",
      " False negatives - incorrect predictions: 260\n",
      "Threshold setting = 0.90\n",
      " True positives  - correct predictions:   27\n",
      " True negatives  - correct predictions:   711\n",
      " False positives - incorrect predictions: 2\n",
      " False negatives - incorrect predictions: 260\n",
      "Threshold setting = 0.95\n",
      " True positives  - correct predictions:   27\n",
      " True negatives  - correct predictions:   711\n",
      " False positives - incorrect predictions: 2\n",
      " False negatives - incorrect predictions: 260\n"
     ]
    }
   ],
   "source": [
    "# test thresholds, with creation of an output file\n",
    "file_name = 'results.csv'\n",
    "with open(file_name,'w') as file:\n",
    "    line = 'TruePositive,TrueNegative,FalsePositive,FalseNegative,Threshold\\n'\n",
    "    file.write(line)\n",
    "    for t in range(1,20,1):\n",
    "        threshold = float(t) / 20\n",
    "        true_positive = 0\n",
    "        false_positive = 0\n",
    "        true_negative = 0\n",
    "        false_negative = 0\n",
    "        for i in range(len(predictions)):\n",
    "            if y_test[i][1] > 0.1:\n",
    "                # the ground truth is a repeat\n",
    "                if predictions[i][1] >= threshold:\n",
    "                    true_positive += 1\n",
    "                else:\n",
    "                    false_negative += 1\n",
    "            else:\n",
    "                # the ground truth is not a repeat\n",
    "                if predictions[i][1] >= threshold:\n",
    "                    false_positive += 1\n",
    "                else:\n",
    "                    true_negative += 1 \n",
    "        print('Threshold setting = %.2f' % threshold)\n",
    "        print(' True positives  - correct predictions:   %s' % true_positive)\n",
    "        print(' True negatives  - correct predictions:   %s' % true_negative)\n",
    "        print(' False positives - incorrect predictions: %s' % false_positive)\n",
    "        print(' False negatives - incorrect predictions: %s' % false_negative)\n",
    "        line = str(true_positive) + ',' + str(true_negative) + ',' + str(false_positive) + ',' + str(false_negative) \n",
    "        line += ',' + str(threshold) + '\\n'\n",
    "        file.write(line)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item 2 is already a repeat\n",
      "Model will predict this with confidence:  0.5507701\n",
      "Item 7 is already a repeat\n",
      "Model will predict this with confidence:  0.16190197\n",
      "Item 9 is already a repeat\n",
      "Model will predict this with confidence:  0.047519617\n",
      "Item 10 is already a repeat\n",
      "Model will predict this with confidence:  0.33312327\n",
      "Item 19 is already a repeat\n",
      "Model will predict this with confidence:  0.23787631\n",
      "Item 20 is already a repeat\n",
      "Model will predict this with confidence:  0.5188767\n",
      "Item 21 is already a repeat\n",
      "Model will predict this with confidence:  0.27636012\n",
      "Item 22 is already a repeat\n",
      "Model will predict this with confidence:  0.33651233\n",
      "Item 23 is already a repeat\n",
      "Model will predict this with confidence:  0.314802\n",
      "Item 25 is already a repeat\n",
      "Model will predict this with confidence:  0.21586314\n"
     ]
    }
   ],
   "source": [
    "# Make predictions (note that some calls may already be repeats by this time)\n",
    "count = 0\n",
    "for r in range(len(X_predict)):\n",
    "    if y_predict[r][1] > 0.1:\n",
    "        print('Item ' + str(r) + ' is already a repeat')\n",
    "        count += 1\n",
    "        print('Model will predict this with confidence: ', nn_model.predict(X_predict[r:r+1])[0][1])\n",
    "    if count == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Done\n",
    "writelog('Python analysis process completed')\n",
    "print('\\nAnalysis process completed')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
